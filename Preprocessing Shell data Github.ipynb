{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe0c636-0a51-4bc4-9cb1-1159c3ebb3af",
   "metadata": {},
   "source": [
    "# Notebook that is used for preprocessing the data that belongs to FollowTheMoney. Part of the code in this notebook was taken from https://github.com/asreview-ftm-hackathon/Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30265a71-243a-4b16-8ed2-d2aa21d60aa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95b10ad2-4ebf-4da2-b2ba-8d9812bb09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c78f2-555f-404b-b6e6-cb51043c4c62",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455497fe-b2cd-4909-a1b1-38fb9a10f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = pd.read_csv('https://github.com/ftmnl/asr/raw/main/data/allExport.csv', sep='|')     # Used for the first part of the dataset that is on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15e07fed-3d96-48b7-9768-6949f6368c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_new = pd.read_csv(r'C:\\Users\\MichaG\\Documents\\Scriptie\\Data-main\\export_dataset2.csv') # Used for the second part of the dataset that is not on\n",
    "                                                                                               # Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39af3c7a-79ac-44ae-ab3e-f0ee9f7c434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove noise\n",
    "process = process.dropna()\n",
    "\n",
    "process = process.rename(columns = {\"file_name_sort\": \"title\", \"content\": \"abstract\"})\n",
    "\n",
    "remove = ['.DS_Store', 'NaN', 'Readme.md']\n",
    "process = process[~process.title.isin(remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3686ce-4e0f-4e2d-b13d-0dd31951efcc",
   "metadata": {},
   "source": [
    "# Transform Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "450faaea-1da0-49b6-bbc9-74505c282134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstract html to string\n",
    "\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def prettify(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    text = text.replace('\\n', '')\n",
    "    #text = text.translate(translate_table)\n",
    "    return str(text)\n",
    "\n",
    "process.abstract = process.abstract.apply(prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0965f3-7e3b-4e66-98e7-ebaa0fe48b61",
   "metadata": {},
   "source": [
    "# Search formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f839eef-d199-452c-8a75-49a834d2968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract id, type and date from title\n",
    "\n",
    "import re\n",
    "\n",
    "numberlist = []\n",
    "typelist = []\n",
    "datelist = []\n",
    "\n",
    "for title in process.title:\n",
    "    id = re.search('^[0-9\\.]+', title)\n",
    "    if id == None:\n",
    "        numberlist.append(None)\n",
    "    else: \n",
    "        numberlist.append(id.group(0))\n",
    "\n",
    "    processtype = re.search('(?<=[0-9\\_]\\_)[a-z A-Z]+(?=\\_)', title)\n",
    "    if processtype != None:\n",
    "        typelist.append(processtype.group(0))\n",
    "    else:\n",
    "        typelist.append(\"Onbekend\")\n",
    "        \n",
    "    date = re.search(\"[0-9-?]+(?=.pdf$)\", title)\n",
    "    if date == None:\n",
    "        datelist.append(None)\n",
    "    else: \n",
    "        try: datelist.append(datetime.strptime(date.group(0), '%d-%m-%Y'))\n",
    "        except: \n",
    "            try: datelist.append(datetime.strptime(date.group(0), '%-d-%-m-%Y'))\n",
    "            except: datelist.append(date.group(0))\n",
    "\n",
    "process[\"id\"] = numberlist\n",
    "process[\"type\"] = typelist\n",
    "process[\"date\"] = datelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c57198-904c-4ca7-a032-5b3e501e5253",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Remove uninteresting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d833a77e-1611-4f7f-b484-29174dac143d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process.abstract = process.abstract.apply(lambda x: str(x))\n",
    "\n",
    "for index, row in process.iterrows():\n",
    "    if re.search(r'\\bposter\\b', process.loc[index].title): # Remove poster documents as they most likely do not contain useful info\n",
    "        process.drop([index], axis = 0, inplace = True)\n",
    "    elif re.search(r'checksum', process.loc[index].abstract): # Remove any file that still contains the word checksum\n",
    "        process.drop([index], axis = 0, inplace = True)\n",
    "process.abstract = process.abstract.apply(lambda x: re.sub(r'S?s?ubject', 'Onderwerp', x)) # Change the word \"subject\" to \"onderwerp\" for every file to make the data cleaning work for every file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d015f-d830-4a88-b2b4-2ac836007a6b",
   "metadata": {},
   "source": [
    "## Improve date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bbd4f79-1bad-4fda-80a5-de10c66c242c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert normal date column to column with type datetime\n",
    "def tryconvertdate(date):\n",
    "    try:\n",
    "        return pd.to_datetime(date, infer_datetime_format=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "actualdatelist = process.date.apply(tryconvertdate)\n",
    "actualdatelist = actualdatelist.tolist()\n",
    "for i in range(1, len(process)):\n",
    "    if pd.isnull(actualdatelist[i]):\n",
    "        actualdatelist[i] = process.date.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff5ffe47-0a39-4ba4-aef6-866856f2f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle edge cases so the entire column becomes of date time format\n",
    "for i in range(1, len(actualdatelist)):\n",
    "    if not isinstance(actualdatelist[i], datetime.datetime):\n",
    "        if actualdatelist[i] == None:\n",
    "            actualdatelist[i] = pd.Timestamp(None)\n",
    "        elif not re.search(\"([?])\", actualdatelist[i]) == None:## check if entry contains question marks\n",
    "            actualdatelist[i] = pd.to_datetime(actualdatelist[i], errors = 'coerce')\n",
    "        elif not re.search(\"[0-9]{4}[-][0-9]{4}\", actualdatelist[i]) == None:\n",
    "            actualdatelist[i] = pd.to_datetime(actualdatelist[i][0:4])\n",
    "process[\"date\"] = actualdatelist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786be846-acb5-40bd-ad09-8f5a237e6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make types uniform\n",
    "for val in process.index.values:\n",
    "    if process.type.loc[val] == 'correspondentie':\n",
    "        process.loc[val] = 'Correspondentie'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed229399-d97b-4b1b-bcf8-a183b3d2423b",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2010aa46-d025-4e8e-8d41-6395f5a44f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the title\n",
    "def cleanTitle(title):\n",
    "    title = re.sub('^[0-9\\.]+_+[a-z A-Z]+_', '', title)\n",
    "    title = re.sub('[0-9\\-]+.pdf$', '', title)\n",
    "    title = re.sub('.msg_', ' ', title)\n",
    "    return title\n",
    "\n",
    "process.title = process.title.apply(cleanTitle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05295b4d-875f-4a37-b115-a2bc0b5c8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove email-adresses and links from abstract\n",
    "process.abstract = process.abstract.apply(lambda x: re.sub(r'\\S*@\\S*\\s?', '', x))\n",
    "process.abstract = process.abstract.apply(lambda x: re.sub(r'(http|www)\\S+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8bdfa-a28a-4d86-b4c4-12b521e71b81",
   "metadata": {},
   "source": [
    "# Properly split documents and extract texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d9b5455-cbc2-4af4-ab60-289e3831cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdocuments(category):\n",
    "    ''' Function that cleans and splits documents based on their contents.\n",
    "    This Method does two things. First of all it tries to extract only the relevant text from a document.\n",
    "    If the function notices a document consists of more then one document, it will try to split the document while adding\n",
    "    them to the dataframe and removing the old one.\n",
    "    '''\n",
    "    for index, row in category.iterrows():                                                                                                          # We iterate over all rows in the input \n",
    "        occurences = [(m.start(0), m.end(0)) for m in re.finditer(r'\\bOnderwerp\\:? ?\\b',row.abstract)]                                              # Find all documents in the entire document by scanning the amount of times 'Onderwerp' is mentioned    \n",
    "        if len(occurences) > 1:                                                                                                                     # Enter if more than one document in the document was found\n",
    "            maillist = []                                                                                                                           # Create an empty list to store emails in if 'Onderwerp' is found more than once\n",
    "            rowdf = pd.DataFrame(category[0:0])\n",
    "            full_mail = ''# Create empty dataframe with correct column names to store emails in temporarily\n",
    "            for i in range(0,len(occurences)):                                                                                                      # For every document found within the document\n",
    "                rowdf = rowdf.append(row)                                                                                                           # Since only the abstracts of each row need to be changed according to the amount of documents found within a document, each time the iteration is entered a new copy of the row is added to the datafram\n",
    "               # Extract the single documents based on indices. By using regex, each document is scanned for the end(by using either Groet or Hoogachend). Then depending on which is found earlier. The starting index of that word is used.\n",
    "                index1, index2 = None,None                                                                                                          # Create empty variables to store indexes in\n",
    "                mail = row.abstract[occurences[i][1]:]                                                                                              # Find beginning of document\n",
    "                if re.search(r'\\bg?G?roete?n?\\:? ?\\b', mail):                                                                                       # Find possible end of document\n",
    "                    index1 = re.search(r'\\bg?G?roete?n?\\:? ?\\b', mail).span() \n",
    "                if re.search(r'\\b.?h?H?oogachtend\\:? ?,?\\b', mail):                                                                                 # Find possible end of document\n",
    "                    index2 = re.search(r'\\b.?h?H?oogachtend\\:? ?,?\\b', mail).span()\n",
    "\n",
    "                if index1 is not None and index2 is not None and index1[0] < index2[0]:                                                             # If both ends were found in a single document, get the end with the lowest beginning index value\n",
    "                    maillist.append(mail[:index1[1]])\n",
    "                elif index1 is not None and index2 is not None and index1[0] > index2[0]:\n",
    "                    maillist.append(mail[:index2[1]])\n",
    "                elif index1 is not None:\n",
    "                    maillist.append(mail[:index1[1]])\n",
    "                elif index2 is not None:\n",
    "                    maillist.append(mail[:index2[1]])\n",
    "                else:                                                                                                                               # If none of the identifiers were found, simply append the document\n",
    "                    maillist.append(mail)\n",
    "            if maillist:                                                                                                                            # Append all seperate changed abstract to a new list which then is added to the main inputted document. The original document is also removed.\n",
    "                #category = category[category['id'] != row.id]\n",
    "                for j in range(0, len(maillist)):\n",
    "                    full_mail += maillist[j]\n",
    "                #full_mail = rowdf.iloc[j,1]\n",
    "                category.loc[index, 'abstract'] = full_mail \n",
    "                #category = pd.concat([category, rowdf])\n",
    "        else:                                                                                                                                       # Only reached if document only consists of a single document, or if the document contains some edge case that was not defined. Does the rest as the for loop above.\n",
    "            index5, index6, final_abstract = None, None, None\n",
    "            temp_abstract = row.abstract\n",
    "            if re.search(r'\\bOnderwerp\\:? ?\\b', temp_abstract):\n",
    "                temp_abstract = temp_abstract[re.search(r'\\bOnderwerp\\:? ?\\b', temp_abstract).span()[1]:]     \n",
    "            if re.search(r'\\bg?G?roete?n?\\:? ?\\b', temp_abstract):\n",
    "                index5 = re.search(r'\\bg?G?roete?n?\\:? ?\\b', temp_abstract).span()\n",
    "            if re.search(r'\\b.?h?H?oogachtend,?\\b', temp_abstract):\n",
    "                index6 = re.search(r'\\b.?h?H?oogachtend,?\\b', temp_abstract).span()\n",
    "                \n",
    "            if index5 is not None and index6 is not None and index5[0] < index6[0]:\n",
    "                final_abstract = temp_abstract[:index5[1]]\n",
    "            if index5 is not None and index6 is not None and index5[0] > index6[0]:\n",
    "                final_abstract = temp_abstract[:index6[1]]\n",
    "            elif index5 is not None:\n",
    "                final_abstract = temp_abstract[:index5[1]]\n",
    "            elif index6 is not None:\n",
    "                final_abstract = temp_abstract[:index6[1]]\n",
    "            \n",
    "            if not re.search(r'\\bOnderwerp\\:? ?\\b', row.abstract):                                                                                 # If none of the identifiers are found simply append the document\n",
    "                final_abstract = temp_abstract\n",
    "            if final_abstract is not None:\n",
    "                category.loc[index, 'abstract'] = final_abstract  \n",
    "            \n",
    "    return category\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36591b22-e76e-44d7-a190-c25fe7862900",
   "metadata": {},
   "source": [
    "## Clean subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae0a7603-ce30-4869-b51d-9a620ebb0da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the previously created function to the relevant subsets\n",
    "correspondentie = process[process['type'] == 'Correspondentie'].copy()\n",
    "edited_correspondentie = splitdocuments(correspondentie)\n",
    "\n",
    "mail = process[process['type'] == 'Mail'].copy()\n",
    "edited_mail = splitdocuments(mail)\n",
    "\n",
    "document = process[process['type'] == 'Document'].copy()\n",
    "document.drop([1925], axis=0, inplace=True) # Column dropped due to very long abstract\n",
    "for val in document.index.values:\n",
    "    if re.search(r'Offerte', document.abstract.loc[val]): # Offertes are not interesting for text analysis so the abstracts containing this term are dropped\n",
    "        document.drop([val], axis=0, inplace=True)\n",
    "\n",
    "edited_document = splitdocuments(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e4189-1562-4259-bad1-809f06b16504",
   "metadata": {},
   "source": [
    "# Concatenate and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45ea5800-6595-48dc-83b4-7f6000ad7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the previously created subsets and extract them so that they can easily be loaded in into another python script\n",
    "finaldf = pd.concat([edited_correspondentie, edited_mail, edited_document])\n",
    "finaldf['abstract_length' ] = finaldf.abstract.apply(lambda x: len(x))\n",
    "finaldf.abstract = finaldf.abstract.apply(lambda x: re.sub(r'\\d+', \"\",x)) #remove digits from abstract\n",
    "finaldf.abstract =  finaldf.abstract.apply(lambda x: re.sub(r'(?:^| )\\w(?:$| )', ' ', x).strip()) #remove single letters surrounded by spaces\n",
    "finaldf = finaldf.reset_index()\n",
    "finaldf[['id','type','title','abstract', 'abstract_length']].to_excel(r'preprocessed_final_nosplit.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
