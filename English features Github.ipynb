{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3c4312-1a3d-4b91-809c-be17639e11e7",
   "metadata": {},
   "source": [
    "# In this notebook the feature generation and passive learning for the Enron dataset is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ae49f-d0c1-4cdb-be0c-ec4a50ba5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 #For data science purposes\n",
    "import matplotlib.pyplot as plt     #For plotting\n",
    "import os\n",
    "from email.parser import Parser     #For parsing enron emails\n",
    "from tqdm import tqdm, tqdm_pandas  #For loading bars                   \n",
    "import re                           #For performing regex\n",
    "import numpy as np\n",
    "import torch                        #For running models with cuda\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification, pipeline\n",
    "import pickle                       #For importing saved variables\n",
    "from nltk import data, pos_tag\n",
    "import nltk \n",
    "from PassivePySrc import PassivePy  #For detecting passive voice sentences\n",
    "import enchant                      #For checking english words\n",
    "from gensim.parsing.preprocessing import remove_stopwords #For removing stopwords\n",
    "import spacy  \n",
    "import mmh3\n",
    "from tensorflow.keras.preprocessing.text import hashing_trick\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "                            \n",
    "tqdm.pandas()\n",
    "\n",
    "# Folder Path\n",
    "path = r\"enron_with_categories\"\n",
    "  \n",
    "# Change the directory\n",
    "os.chdir(path)\n",
    "\n",
    "emaillist = []\n",
    "# Read text File\n",
    "          \n",
    "# Extract email from a txt file  \n",
    "def extract_text_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        if file_path.endswith(\".txt\"):\n",
    "            data = f.read()\n",
    "            email = Parser().parsestr(data)\n",
    "            body = email.get_payload()\n",
    "            return body\n",
    "# Extract labels from .cats file \n",
    "def extract_labels(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        if file_path.endswith(\".cats\"):\n",
    "            return(f.read().splitlines())\n",
    "\n",
    "# By using the above two methods, create a dataframe and extract labels and text from emails. Then concatenate these into one dataframe.        \n",
    "column_names = [\"abstract\", \"label\", \"length\"]\n",
    "enron_df = pd.DataFrame(columns = column_names)\n",
    "for root, dirs, files in os.walk(r\"enron_with_categories\"):\n",
    "    for file in files:\n",
    "        numericlabel, label, email, length, filename = None,None,None,None,None  # Create empty variables for storing results\n",
    "        if file.endswith('.txt'):\n",
    "            email = extract_text_from_file(os.path.join(root,file))\n",
    "            email = ' '.join(email.split()[:len(email.split())])\n",
    "            length = len(email)\n",
    "            filename = file.rpartition('.')[0]\n",
    "            for catfile in files:\n",
    "                if catfile.endswith('.cats') and catfile.rpartition('.')[0] == file.rpartition('.')[0]:\n",
    "                    numericlabel = extract_labels(os.path.join(root,catfile))\n",
    "                    for labelset in numericlabel:\n",
    "                        if '1,4' in labelset: #This can be changed to import whatever type of label that one would like\n",
    "                            label = 1\n",
    "                            break\n",
    "                        elif not label == 1:\n",
    "                            label = 0 \n",
    "            if length < 15000 and length > 15: # Remove very long and short emails\n",
    "                enron_df = pd.concat([pd.DataFrame.from_records([{'abstract': email, 'label': label, 'length' : length, 'title' : filename}]), enron_df], ignore_index=True)\n",
    "        \n",
    "enron_df = enron_df.astype({'label': 'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ea44b-1f8d-44dc-8d5e-2eafd495d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MichaG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MichaG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MichaG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and download NLTK packages\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77759c-456d-4cc4-8d33-4f728f1ee196",
   "metadata": {},
   "source": [
    "# Feature generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0157f-fd2a-4a2e-85c5-3fd6d95676a4",
   "metadata": {},
   "source": [
    "## Sentiment values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "798aaf31-332f-487d-a036-aaf7042c1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and tokenizer for sentiment analysis\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\" ) # Load in model for sentiment analysis\n",
    "model.eval() # Make sure model is not in training mode anymore\n",
    "tokenizernlp = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", model_max_len=512) # Load in tokenizer and set max_len to 512 as graphics card can't handle more than that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cc2385e6-6566-4311-a669-d3694680a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1597 [00:00<01:27, 18.13it/s]C:\\Users\\MichaG\\Anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:997: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 1597/1597 [01:28<00:00, 18.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform sentiment analysis on all rows\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\" , model=model, tokenizer = tokenizernlp, device = 0) # Create pipeline for performing sentiment analysis\n",
    "def generatesentimentvalues(text):\n",
    "    sentiment_result = sentiment_analysis(text, **tokenizer_kwargs) # perform sentiment analysis on text\n",
    "    if sentiment_result[0]['label'] == 'NEGATIVE': # As we can only use numbers for the final model, we need to transform the results from numbers with a label to purely numbers\n",
    "        result = 0-sentiment_result[0]['score']\n",
    "    else:\n",
    "        result = sentiment_result[0]['score']\n",
    "    return result\n",
    "enron_df['sentiment_score'] = enron_df.abstract.progress_apply(lambda x: generatesentimentvalues(x)) #Apply sentiment analysis on dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350a102-bf8b-4ed0-a060-364fb73fb44b",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c80a20-fb15-4115-b7fe-15b9cba7c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and tokenizer for named entity recognition\n",
    "modelner = AutoModelForTokenClassification.from_pretrained('xlm-roberta-large-finetuned-conll03-english', return_dict=True)\n",
    "modelner.eval()\n",
    "#modelner = modelner.to(\"cuda:0\")\n",
    "\n",
    "tokenizerner = AutoTokenizer.from_pretrained('xlm-roberta-large-finetuned-conll03-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "802b502d-ebd9-4105-b6b1-a029374c43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "def generate_named_entities(text):\n",
    "    ''' Function that generates named entity values for the inputted text.\n",
    "    This Method does a few things. First it splits the text into single sentences(split_into_senteces). The short sentences are then removed based on the average length of the sentences in the text(remove_short_tokens)\n",
    "    The tokens are then fed into a tokenizer and the generated tokens are fed into a model that generates named entities based on the tokens. The result of this is returned as a dict which can then be appended to the dataframe.\n",
    "    them to the dataframe and removing the old one.\n",
    "    '''\n",
    "    tokens = [x for x in split_into_sentences(text) if not any(y in x for y in ['/','+'])] # split text into sentences and remove any sentence that contains / or + as a character\n",
    "    tokens = remove_short_tokens(tokens)\n",
    "    if tokens:\n",
    "        inputs = tokenizerner.batch_encode_plus(tokens, return_tensors=\"pt\", padding = True, max_length = 512, truncation = True) # tokenize sentences, max_length is 512 for if cuda is enabled to speed the model up\n",
    "        with torch.no_grad():\n",
    "            results = modelner(**inputs)\n",
    "            for i, input in enumerate(inputs['input_ids']):\n",
    "                namedentities = [modelner.config.id2label[item.item()] for item in results.logits[i].argmax(axis=1)] #for every probability for a named entity for a word, turn the probabilities into their associated labels\n",
    "        entitynumberlist = generate_entity_list(namedentities) #Based on the array of entity names that is generated, count each entity and make a dict of this\n",
    "    else:\n",
    "        entitynumberlist = {'B-LOC': 0, 'B-MISC': 0, 'B-ORG' : 0, 'I-LOC' : 0, 'I-MISC': 0, 'I-ORG': 0, 'I-PER': 0}\n",
    "    return entitynumberlist\n",
    "\n",
    "def remove_short_tokens(tokens):\n",
    "    average = 0\n",
    "    for token in tokens:\n",
    "        average += len(token)\n",
    "    try:      \n",
    "        average = average/len(tokens)\n",
    "        return([x for x in tokens if len(x) >= average])\n",
    "    except:\n",
    "        return(tokens)\n",
    "    \n",
    "def generate_entity_list(entities):\n",
    "    B_LOC, B_MISC, B_ORG, I_LOC, I_MISC, I_ORG, I_PER = 0,0,0,0,0,0,0\n",
    "    for entity in entities:    \n",
    "        if entity == 'B-LOC':\n",
    "            B_LOC += 1\n",
    "        elif entity == 'B-MISC':\n",
    "            B_MISC += 1\n",
    "        elif entity == 'B-ORG':\n",
    "            B_ORG += 1\n",
    "        elif entity == 'I-LOC':\n",
    "            I_LOC += 1\n",
    "        elif entity == 'I-MISC':\n",
    "            I_MISC += 1\n",
    "        elif entity == 'I-ORG':\n",
    "            I_ORG += 1\n",
    "        elif entity == 'I-PER':\n",
    "            I_PER += 1\n",
    "    return({'B-LOC': B_LOC, 'B-MISC': B_MISC, 'B-ORG': B_ORG, 'I-LOC': I_LOC, 'I-MISC': I_MISC, 'I-ORG':I_ORG, 'I-PER':I_PER})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80844e9d-b5d2-4834-8d74-14b00ecd2621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Gnerate seperate named entity recognition dataframe that contains all entities found in the texts\n",
    "col_names_entity_df1 =  [\"B-LOC\", \"B-MISC\", \"B-ORG\", \"I-LOC\", \"I-MISC\", \"I-ORG\", \"I-PER\"]\n",
    "entity_df1 = pd.DataFrame(columns = col_names_entity_df1)\n",
    "entity_df1 = entity_df1.append({'B-LOC': 0, 'B-MISC': 0, 'B-ORG' : 0, 'I-LOC' : 0, 'I-MISC': 0, 'I-ORG': 0, 'I-PER': 0},ignore_index = True)\n",
    "#entity_df1 = entity_df1.append(enron_df.abstract.progress_apply(lambda x: generate_named_entities(x)), ignore_index = True) \n",
    "entity_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30f01d-9198-4624-9c05-956a3936f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the structure of the previous dataframe went wrong, extract the information of the dataframe and put it into a proper dataframe\n",
    "col_names_entity_df2 =  [\"B-LOC\", \"B-MISC\", \"B-ORG\", \"I-LOC\", \"I-MISC\", \"I-ORG\", \"I-PER\"]\n",
    "entity_df2 = pd.DataFrame(columns = col_names_entity_df2)\n",
    "for column in entity_df1:\n",
    "    entity_df2 = entity_df2.append(entity_df1.iloc[0][column], ignore_index=True)\n",
    "\n",
    "entity_df2['filename'] = enron_df.filename\n",
    "enron_df = pd.merge(enron_df, entity_df2, on='filename', how='inner')\n",
    "enron_df[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4259a6f9-5adc-4c79-aa35-3a1e86c7c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the named entity recognition is very slow on the cpu, and is too big for the gpu, a saved version of the variable is loaded in using pickle and appended to the original dataframe.\n",
    "with open(r'NERframe.txt', 'rb') as f:\n",
    "    entity_df = pickle.load(f)\n",
    "\n",
    "enron_df = pd.merge(enron_df, entity_df, left_on='title', right_on='filename', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39420940-223f-4336-9d83-25a9aba95ebe",
   "metadata": {},
   "source": [
    "## Other features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2aa2e6-2aca-4b5e-8197-e91769e5ba2a",
   "metadata": {},
   "source": [
    "### Specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "46e6bbc1-a128-4b76-ae40-6028d9e79a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def specific_words_check(text):\n",
    "    ''' Function that searches for specific words and sums the total occurences\n",
    "    '''\n",
    "    amount_of_words = len(re.findall(r'(\\b[Mm]+eet?(ing)?s?\\b|\\b[Pp]+lane\\b|\\bexpense report\\b|\\b[Cc]+all\\b|\\b[Vv]+oicemail\\b|\\b[Ee]+?[Mm]+ail(ing)?\\b|\\b[Ww]+eeks\\b|\\b([Ss]+chedul(e)?(ing)?)|\\b[Tt]+ime|\\b[Ww]+eek\\b|\\b[Ii]+nvite?d?(ing)?\\b|\\b([0-1]?[0-9]):[0-5][0-9]\\b)', text))\n",
    "    if amount_of_words:\n",
    "        return(amount_of_words)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0187a996-8187-49aa-a149-ca00c55529d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_df['specific_words_counter'] = enron_df.abstract.apply(lambda x: specific_words_check(x)) #apply the previous function using a lambda statement to create a new column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4e5bf-f9f6-4d13-8d20-90c5370ec7cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Standard deviation of sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eeceafd8-f860-44ee-9008-56bdcb8a3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_dev_sentence_length(text):\n",
    "    ''' Function that calulates the standard deviation of the length of all the sentences in a text.\n",
    "    '''\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentence_length = []\n",
    "    for item in sentences:\n",
    "        sentence_length.append(len(item))\n",
    "    return(np.std(sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dbbd21f1-27d4-4062-99d2-484685ab87ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1597/1597 [00:00<00:00, 2860.47it/s]\n"
     ]
    }
   ],
   "source": [
    "enron_df['stdev_sentence_length'] = enron_df.abstract.progress_apply(lambda x: standard_dev_sentence_length(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bae11-0d9a-4df7-8eb9-07857e53a5af",
   "metadata": {},
   "source": [
    "### Automated readability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7c6affe2-441f-43d7-af9c-69348fe888fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_index(text):\n",
    "    ''' Function that calculates the automated readability index of a text.\n",
    "    '''\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    words = text.count(' ')\n",
    "    characters = len(text) - words\n",
    "    try:\n",
    "        return(4.71*(characters/words) + 0.5*(words/len(sentences)) -21.43)\n",
    "    except:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "01141221-5e87-4b38-bcdd-20db197a0080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1597/1597 [00:00<00:00, 3406.37it/s]\n"
     ]
    }
   ],
   "source": [
    "enron_df['readability_index'] = enron_df.abstract.progress_apply(lambda x: readability_index(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed6c42-534d-478d-9814-e4db530b3066",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Standard deviation of word lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d4bf19c0-c4ef-4044-8b81-3a0871329ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standard_dev_word_length(text):\n",
    "    ''' Function that calculates the standard deviation of the word lengths in a text.\n",
    "    '''\n",
    "    words = text.split()\n",
    "    words_length = []\n",
    "    for word in words:\n",
    "        words_length.append(len(word))\n",
    "    return(np.std(words_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4b012f75-75e5-481c-ba6a-ceb8f5ba7b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1597/1597 [00:00<00:00, 10765.24it/s]\n"
     ]
    }
   ],
   "source": [
    "enron_df['stdev_word_length'] = enron_df.abstract.progress_apply(lambda x: standard_dev_word_length(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b0410-5089-4b1f-bb49-4a40e83f6cf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Type Token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee0eb94-e50d-4337-bc81-f0add1c59667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def type_token_ratio(text):\n",
    "    ''' Function that calculates the type token ratio of the text. The type token ratio is the ratio between the toal amount of words and the amount of unique words in a text.\n",
    "    '''\n",
    "    unique = set(text.split()) \n",
    "    return len(unique)/ len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8695b1a9-aeb9-41a6-afa1-36c734b6aa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1597/1597 [00:00<00:00, 12163.69it/s]\n"
     ]
    }
   ],
   "source": [
    "enron_df['type_token_ratio'] = enron_df.abstract.progress_apply(lambda x:type_token_ratio(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5e255-6914-4e21-a5d9-6c61d4aa8d3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e2d884-d8ab-4342-b340-3456f0bdb69e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def proper_nouns(text):\n",
    "    ''' Function that caclulates how many proper nouns there are in a text. This is done with the help of the NLTK perceptron tagger.\n",
    "    '''\n",
    "    tagged_sent = pos_tag(text.split())\n",
    "    propernouns = [word for word,pos in tagged_sent if pos == 'NNP']\n",
    "    return len(propernouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f4635-d0f8-4d4c-9317-e44ba334f3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1597/1597 [00:25<00:00, 61.86it/s]\n"
     ]
    }
   ],
   "source": [
    "enron_df['proper_nouns'] = enron_df.abstract.progress_apply(lambda x: proper_nouns(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a73344-d829-4242-836d-42e314db80a7",
   "metadata": {},
   "source": [
    "### Passive voice sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da49326-4b39-4b56-9951-9bde835de830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MichaG\\Anaconda3\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_core_web_lg' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "spacy_model = \"en_core_web_lg\"\n",
    "passivepy = PassivePy.PassivePyAnalyzer(spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b628519c-20c9-46db-aa01-e90acff5047b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1597/1597 [02:35<00:00, 10.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def percentage_passive_voice(text):\n",
    "    ''' Function that caclulates what percentage of sentences in a text are written in passive voice. This is done using the passivePy package.\n",
    "    '''\n",
    "    passive_amount = passivepy.match_text(text, full_passive=True, truncated_passive=True).passive_count.iloc[0]\n",
    "    sentence_amount = nltk.tokenize.sent_tokenize(text)\n",
    "    return passive_amount/len(sentence_amount)\n",
    "enron_df['passive_voice_%'] = enron_df.abstract.progress_apply(lambda x: percentage_passive_voice(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e20ef-cd58-4d65-909d-23a7b6273571",
   "metadata": {},
   "source": [
    "### Active voice sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0a62d8e1-3b51-481b-8bfd-b1194ca769df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596/1596 [02:03<00:00, 12.94it/s]\n"
     ]
    }
   ],
   "source": [
    "def percentage_active_voice(text):\n",
    "    ''' Function that caclulates what percentage of sentences in a text are written in active voice. This is done using the passivePy package.\n",
    "    '''\n",
    "    return(1-percentage_passive_voice(text))\n",
    "enron_df['active_voice_%'] = enron_df.abstract.progress_apply(lambda x: percentage_active_voice(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3381704-cb11-4241-917f-4cf3e16da565",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c1fbf-efeb-4cba-97f8-9ac74960f249",
   "metadata": {},
   "source": [
    "### preprocessing tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390d4bb0-8bb9-48ff-9452-5edba2bb59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(text):\n",
    "    return np.char.lower(text)\n",
    "def remove_email_adresses(text):\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b4f0a03-eab0-44ed-8864-8294bf27a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99b0d6a0-59a2-4828-8bd4-923cc39acb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19ccec86-6ba3-4353-ab5a-96d25d71b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da8b4bf0-d23c-4b2d-8c97-6c79dd66520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "def remove_random_words(data):\n",
    "    data = str(data)\n",
    "    data = re.sub(r'\\b(USL)?(usl)?\\b', '', data)\n",
    "    data = re.sub(r'\\b(\\.((DOC)?(doc)?))\\b', '', data)\n",
    "    data = re.sub(r'\\b(e-mail)\\b', '', data)\n",
    "    data = re.sub(r'(\\<div\\>)', '', data)\n",
    "    data = re.sub(r'(\\<br\\>)', '', data)\n",
    "    data = re.sub(r'(.*?\\.[\\w:]+)', '', data)\n",
    "    data = re.sub(r'\\[image\\]', '', data)\n",
    "    #data = re.sub(r'\\b3d\\b', '', data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b2e7e8e-e13e-4313-8890-f17234111da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers_phonenumbers(text):\n",
    "    ''' Function that remove specific sequences of numbers from a text so that they are not seen as words by the BagofWords feature.\n",
    "    '''\n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\b([0-9]{3}-[0-9]{3}-[0-9]{4})\\b', '', text) #Remove US phone numbers\n",
    "    text = re.sub(r'\\b([0-1][0-9]\\/[0-3][0-9]\\/[0-9]{4})\\b', '', text) #Removes dates\n",
    "    text = re.sub(r'\\b([0-1]?[0-9]):[0-5][0-9]\\b', '', text) #Removes timestamps\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text) #Removes single whitespaces\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4050c216-b566-4f03-8565-91266759f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_email_adresses(data)\n",
    "    data = remove_random_words(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = remove_numbers_phonenumbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = remove_numbers_phonenumbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4ee1c9f-b609-4a8b-9ae2-47d36186560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1597/1597 [01:36<00:00, 16.55it/s]\n"
     ]
    }
   ],
   "source": [
    "enron_df_bow = enron_df.copy() #Create copy so that we dont modify the actual dataframe\n",
    "\n",
    "enron_df_bow.abstract = enron_df.abstract.progress_apply(lambda x: word_tokenize(str(preprocess(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be8c57e8-6323-4a36-9d35-e09758f6cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "for i in range(0,len(enron_df_bow)):\n",
    "    tokens = enron_df_bow.abstract.iloc[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3b65c26-5c90-45d0-b455-e5a073d0b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87d5b7cd-63b1-4038-a99b-9062ab50953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1597/1597 [00:00<00:00, 1923.99it/s]\n"
     ]
    }
   ],
   "source": [
    "N = len(enron_df_bow)\n",
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "df_tfidf = pd.DataFrame({'Doc': pd.Series(dtype='str'),\n",
    "                   'term': pd.Series(dtype='str'),\n",
    "                   'value': pd.Series(dtype='float'),\n",
    "                   'label': pd.Series(dtype='int')})\n",
    "for i in tqdm(range(0,N)):\n",
    "    tokens = enron_df_bow.abstract.iloc[i]\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "        #df_tfidf.loc[len(df_tfidf)] = [doc,token,tf*idf, enron_df.loc[doc].label]\n",
    "\n",
    "    doc += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603fbd6-161d-4967-90ca-0b0cf46eb4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame({'Doc': pd.Series(dtype='str'),\n",
    "                   'term': pd.Series(dtype='str'),\n",
    "                   'value': pd.Series(dtype='float')})\n",
    "for key in tf_idf:\n",
    "    new_row = pd.DataFrame({'Doc':[key[0]], 'term':[key[1]], 'value': [tf_idf[key]]})\n",
    "    df_tfidf = pd.concat([new_row,df_tfidf.loc[:]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831cbbe-f6de-4a88-9a9c-b2eb4b830dc1",
   "metadata": {},
   "source": [
    "### Bag of words with hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720afc4-5047-4c04-99f3-f0c2d4065ed8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### preprocessing bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de44f448-1181-4924-bf55-758fb3b8171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1597/1597 [01:30<00:00, 17.59it/s]\n"
     ]
    }
   ],
   "source": [
    "enron_df_bow = enron_df.copy()\n",
    "enron_df_bow['abstract'] = enron_df_bow.abstract.progress_apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d291bc69-f0ca-4874-a5b9-d12344774955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hashed_BOW(text, length):\n",
    "    '''With this function hashes of texts can be generated\n",
    "    '''\n",
    "    bow_array = [0]*length\n",
    "    hashes = hashing_trick(\n",
    "        text,\n",
    "        length,\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "        lower=True,\n",
    "        split=' ',\n",
    "        analyzer=None)\n",
    "    for hash_ in hashes:\n",
    "        bow_array[hash_] = bow_array[hash_]+1\n",
    "    \n",
    "    return bow_array[0:length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dc38f32-2408-4af5-9ea4-2227d49a3122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell a dataframe of all bag of words values is created\n",
    "BOW_df = pd.DataFrame(columns=[str(i) for i in range(0,1001)])\n",
    "\n",
    "for i in range(len(enron_df_bow)):\n",
    "    BOW_values = Hashed_BOW(enron_df_bow.abstract.loc[i], 1001)\n",
    "    BOW_df.loc[i] = BOW_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22d62fd6-64cd-4c08-99a3-6235c70669ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MichaG\\AppData\\Local\\Temp\\ipykernel_8072\\2187400621.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  BOW_df[str(hashing_trick(key[1], 1001, lower=True)[0])].iloc[key[0]] = BOW_df[str(hashing_trick(key[1], 1001, lower=True)[0])].iloc[key[0]]*tf_idf[key]\n"
     ]
    }
   ],
   "source": [
    "# In this cell the tf-idf values are applied to the bag of words values\n",
    "for key in tf_idf:\n",
    "    BOW_df[str(hashing_trick(key[1], 1001, lower=True)[0])].iloc[key[0]] = BOW_df[str(hashing_trick(key[1], 1001, lower=True)[0])].iloc[key[0]]*tf_idf[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118cdc6-996b-4b97-ad02-328e5435aaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enron_df_final = pd.concat((enron_df, BOW_df), axis = 1) #Concatenate the BagOfWords dataframe with the other dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a1f2cc8-e357-495f-8254-fc349e25fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_df_final = enron_df.copy() # If no bag of words exist, make the enron_df_final variable a copy of the frame that contained the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a1eb6-a929-4a53-bbf0-3b76c6e8019a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With this cell feature histograms for feature selection can be generated\n",
    "plot = enron_df_final.groupby(pd.cut(enron_df_final.label, bins = 2))['type_token_ratio'].mean().plot.bar()\n",
    "plot.figure.savefig(r'C:\\Users\\MichaG\\Pictures\\Scriptie\\typetokenratio.pdf') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed0b5d0-6cf7-483d-b1fb-72db4b73e41c",
   "metadata": {},
   "source": [
    "## Creating subset for testing asreview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "e0f71505-28ce-4046-bc32-4377b9d92769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By using train test split with the setting stratify we can create a small labeled set in the dataset and a larger unlabeled set which can then be used for active learning in ASReview\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(enron_df_final.drop('label',axis=1), \n",
    "                                                    enron_df_final['label'], stratify= enron_df_final['label'], test_size=217, random_state = 29)\n",
    "\n",
    "enron_df_subset = X_train.copy()\n",
    "enron_df_subset['label'] = y_train\n",
    "enron_df_subset1 = X_test.copy()\n",
    "enron_df_subset1['label'] = np.nan\n",
    "enron_test_export = pd.concat([enron_df_subset,enron_df_subset1])\n",
    "enron_test_export = enron_test_export.drop(['length'], axis = 1)\n",
    "\n",
    "enron_test_export[['abstract', 'label','title']].to_excel(r'enron_logistic_df_unbalanced_with_title_small.xlsx')\n",
    "#enron_df.to_csv(r\"C:\\Users\\MichaG\\Documents\\Scriptie\\Data-main\\enron.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed5148-fc9e-4c37-b70a-69809609bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split for when using neural networks. Currently not using this\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(enron_df_final.drop('label',axis=1), \n",
    "                                                    enron_df_final['label'], stratify= enron_df_final['label'], test_size=0.1)\n",
    "enron_train_df = pd.concat([X_train.copy(), y_train.copy()], axis = 1)\n",
    "enron_train_df.drop(['abstract', 'title', 'filename','B-LOC', 'B-MISC', 'B-ORG'], axis=1, inplace = True)\n",
    "enron_test_df = pd.concat([X_test.copy(), y_test.copy()], axis = 1)\n",
    "enron_test_df.drop(['abstract', 'title', 'filename','B-LOC', 'B-MISC', 'B-ORG'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848e5c6-cea7-488a-857b-30a6d6b5b4ce",
   "metadata": {},
   "source": [
    "## Simulation subset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6a64a370-d400-44b2-943f-0d3c80f24c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_df_final[['abstract', 'label', 'title']].to_excel(r'enron_simulation_data_bussines.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217d1fa-e9a9-4902-a238-9a16ae814805",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a3abe69-2751-4543-b2cf-cec1786ce111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>passive_voice_%</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>1941</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.639610</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>3697</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.562832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014896</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>1602</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>1.674147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>6557</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.456670</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>561</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>682</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202526</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>9992</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.551597</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.005539</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>3503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628366</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1380 rows × 1004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     length  passive_voice_%  type_token_ratio  0         1         2  \\\n",
       "1247   1941         0.214286          0.639610  0  0.000000  0.000000   \n",
       "457    3697         0.153846          0.562832  0  0.000000  0.000000   \n",
       "1447   1602         0.083333          0.614286  0  0.000000  0.000000   \n",
       "138     938         0.000000          0.734694  0  0.000000  0.000000   \n",
       "320     708         0.000000          0.938144  0  0.000000  0.000000   \n",
       "...     ...              ...               ... ..       ...       ...   \n",
       "978    6557         0.351351          0.456670  0  0.000000  0.000000   \n",
       "899     561         0.500000          0.825000  0  0.000000  0.000000   \n",
       "9       682         0.500000          0.784091  0  0.000000  0.000000   \n",
       "859    9992         0.120690          0.551597  0  0.000059  0.000553   \n",
       "1117   3503         0.000000          0.628366  0  0.000000  0.000000   \n",
       "\n",
       "             3    4         5         6  ...       991  992  993  994  \\\n",
       "1247  0.000000  0.0  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0   \n",
       "457   0.043002  0.0  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0   \n",
       "1447  0.000000  0.0  0.001713  1.674147  ...  0.000000  0.0  0.0  0.0   \n",
       "138   0.010930  0.0  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0   \n",
       "320   0.009876  0.0  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0   \n",
       "...        ...  ...       ...       ...  ...       ...  ...  ...  ...   \n",
       "978   0.000000  0.0  0.000000  0.000000  ...  0.000003  0.0  0.0  0.0   \n",
       "899   0.000000  0.0  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0   \n",
       "9     0.000000  0.0  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0   \n",
       "859   0.000000  0.0  0.000000  0.000000  ...  0.000056  0.0  0.0  0.0   \n",
       "1117  0.000000  0.0  0.000000  0.017558  ...  0.000000  0.0  0.0  0.0   \n",
       "\n",
       "           995       996       997       998       999      1000  \n",
       "1247  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "457   0.000000  0.000000  0.000000  0.000000  0.014896  0.000000  \n",
       "1447  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "138   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "320   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "978   0.000059  0.006986  0.004146  0.000000  0.000000  0.009414  \n",
       "899   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "9     0.000000  0.000000  0.000000  0.000000  0.202526  0.000000  \n",
       "859   0.002509  0.000000  0.000029  0.005539  0.000191  0.000000  \n",
       "1117  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[1380 rows x 1004 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performing testing of the model. By using stratify we can simulate training the model using a much small dataset and we can make sure that the ratio of labels in the train and test set is the same as in the orignal dataset\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE \n",
    "from sklearn.model_selection import train_test_split  #For oversampling\n",
    "\n",
    "#enron_df_lr = enron_df_final[top50]\n",
    "#enron_df_lr = enron_df_final.drop(['abstract', 'title', 'filename','B-LOC', 'B-MISC', 'B-ORG', 'length'], axis=1) #Create copy so we don't modify the original dataframe\n",
    "enron_df_lr = enron_df_final.drop(['abstract', 'title'], axis=1) #Create copy so we don't modify the original dataframe\n",
    "\n",
    "\n",
    "enron_df_small = None\n",
    "X_train, X_test, y_train, y_test = train_test_split(enron_df_lr.drop(['label'],axis=1), \n",
    "                                                    enron_df_lr['label'], test_size= 200, random_state = 29) \n",
    "ros = RandomOverSampler(random_state=0)\n",
    "sm = SMOTE(random_state = 0)\n",
    "X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train) #Oversmampling train test set\n",
    "\n",
    "enron_df_small = X_train.copy()\n",
    "enron_df_small['label'] = y_train\n",
    "\n",
    "#When we want to simulate training with smaller labeled set we can utilise these variables.\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(enron_df_small.drop('label',axis=1), \n",
    "                                                    enron_df_small['label'], stratify = enron_df_small['label'],test_size=17, random_state = 29)\n",
    "X_train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5598389-7fc0-48e9-9462-3a040289b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.748309 using {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.748309 (0.047330) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.747051 (0.047776) with: {'C': 0.9, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.745007 (0.047902) with: {'C': 0.8, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.742150 (0.047330) with: {'C': 0.7, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.736698 (0.047882) with: {'C': 0.6, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.731797 (0.046190) with: {'C': 0.5, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.720024 (0.042338) with: {'C': 0.4, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.703654 (0.042454) with: {'C': 0.3, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.675589 (0.046153) with: {'C': 0.2, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.649023 (0.048977) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.609885 (0.055952) with: {'C': 0.05, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.599434 (0.060069) with: {'C': 0.03, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.600348 (0.060443) with: {'C': 0.025, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.596846 (0.060791) with: {'C': 0.02, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.583926 (0.061941) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76        11\n",
      "           1       0.57      0.67      0.62         6\n",
      "\n",
      "    accuracy                           0.71        17\n",
      "   macro avg       0.69      0.70      0.69        17\n",
      "weighted avg       0.72      0.71      0.71        17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Perform grid search to find optimal configuration. Takes a long time to do since there are 200+ variables\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# define models and parameters\n",
    "logmodel = LogisticRegression(max_iter = 1700, class_weight = 'balanced')\n",
    "solvers = ['liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.03, 0.025, 0.02, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=20, n_repeats=5, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=logmodel, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1_weighted',error_score=0)\n",
    "grid_result = grid_search.fit(X_train_small, y_train_small)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(classification_report(y_test_small, grid_result.best_estimator_.predict(X_test_small)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b1725-61e2-440e-b9f0-5b76d41312d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>colnames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000454</td>\n",
       "      <td>length</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007821</td>\n",
       "      <td>proper_nouns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.805656</td>\n",
       "      <td>passive_voice_%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.104020</td>\n",
       "      <td>sentiment_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000312</td>\n",
       "      <td>stdev_sentence_length</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.008922</td>\n",
       "      <td>readability_index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.099499</td>\n",
       "      <td>stdev_word_length</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.419935</td>\n",
       "      <td>type_token_ratio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef               colnames\n",
       "0 -0.000454                 length\n",
       "1  0.007821           proper_nouns\n",
       "2 -0.805656        passive_voice_%\n",
       "3  0.104020        sentiment_score\n",
       "4 -0.000312  stdev_sentence_length\n",
       "5 -0.008922      readability_index\n",
       "6  0.099499      stdev_word_length\n",
       "7  1.419935       type_token_ratio"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With this cell the values of the coefficients of the features can be presented in a visually appealing way\n",
    "df_coef = pd.DataFrame(pd.Series(grid_result.best_estimator_.coef_[0]), columns=['coef'])\n",
    "df_coef['colnames'] = X_train.columns\n",
    "# Remove bag of words feature from coefficients\n",
    "for index, row in df_coef.iterrows():\n",
    "    if re.search(r'[0-9]', row.colnames):\n",
    "        df_coef.drop([index], inplace=True)\n",
    "df_coef"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
